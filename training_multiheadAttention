🎯 Explanation:
Multi-head attention allows the model to look at the input from multiple "perspectives" or subspaces at once. 
Each head captures different types of relationships (e.g., local, global, positional).

🧠 Real-world analogy:
Think of a detective team investigating a crime scene. One looks for fingerprints, another for footprints, 
another analyzes camera footage. Each finds something different, but together they reconstruct the full picture.


Transformers do not have recurrence like RNNs. So, they use Positional Encoding to inject information about the order of words into the input embeddings.
These encodings are added to token embeddings to help the model know whether a word came first, second, or last.

🧠 Real-world analogy:
Imagine reading a paragraph but with all the words scrambled randomly. You'd struggle to understand it without knowing the original order. 
Positional encoding is like adding timestamps to each word — so even when scrambled, you know which came first.



🔍 Q2: Do RNNs require positional encoding? How do they retain word positions?
🧠 Short Answer:
No, RNNs don’t require explicit positional encodings because they process inputs sequentially — one token at a time — and hence naturally "remember" the order.

🔄 How RNNs Retain Word Order:
RNNs work like this:

Input: [I, love, pizza]
Step 1: RNN processes "I" and updates internal state.
Step 2: It then takes "love", along with the updated state (which remembers "I").
Step 3: Then "pizza", building on the previous state that has "I love".
This internal state carries the memory of all previous tokens, so order is implicitly encoded in the hidden state.

🧠 Real-world Analogy:
Imagine listening to a story one sentence at a time. You remember what happened before (your memory = hidden state), and that helps you understand what comes next.

That’s an RNN: it builds up a mental summary as it reads.

🔍 Q3: Transformers do not have recurrence. So, why do they need Positional Encoding?
🎯 The Transformer Problem:
Transformers process all tokens in parallel, not sequentially. This is amazing for speed and parallel computing. But...

❌ It doesn’t know who came first or which token is where in the sentence.

So we need to explicitly add position info to each token.

✅ Positional Encoding Solution:
We add vectors to each word embedding that encode the position (0th, 1st, 2nd...) using sinusoids or learnable embeddings.

This helps attention layers know the relative positions of tokens.

🔢 Example:
Sentence: "The cat sat"

Token embeddings:

"The" → [0.1, 0.2, 0.4]

"cat" → [0.3, 0.1, 0.5]

"sat" → [0.6, 0.2, 0.7]

Positional encodings:

Position 0 → [0.05, 0.02, 0.01]

Position 1 → [0.1, 0.03, 0.02]

Position 2 → [0.15, 0.04, 0.03]

Final input to transformer = token embedding + positional encoding
(e.g., "cat" at position 1 → [0.4, 0.13, 0.52])

🧠 Real-world Analogy:
Imagine you receive a puzzle with shuffled pieces and no numbers.

You wouldn’t know the order to arrange them.

🟢 Positional Encoding is like numbering the puzzle pieces so you can reconstruct the original picture.

