ðŸŽ¯ Explanation:
Multi-head attention allows the model to look at the input from multiple "perspectives" or subspaces at once. 
Each head captures different types of relationships (e.g., local, global, positional).

ðŸ§  Real-world analogy:
Think of a detective team investigating a crime scene. One looks for fingerprints, another for footprints, 
another analyzes camera footage. Each finds something different, but together they reconstruct the full picture.


Transformers do not have recurrence like RNNs. So, they use Positional Encoding to inject information about the order of words into the input embeddings.
These encodings are added to token embeddings to help the model know whether a word came first, second, or last.

ðŸ§  Real-world analogy:
Imagine reading a paragraph but with all the words scrambled randomly. You'd struggle to understand it without knowing the original order. 
Positional encoding is like adding timestamps to each word â€” so even when scrambled, you know which came first.
